{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GradientChecking.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOD5Depq6X9JsnfFoxqtmGg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noahbisht0/Deep-Learning/blob/main/GradientChecking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CohoQNFm8Cxg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "0ba2400a-429b-4ba8-ada5-a4b80a1540a8"
      },
      "source": [
        "import numpy as np \n",
        "from testCases import * \n",
        "from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector\n",
        "def forward_propagation(x,theta):\n",
        "  J = np.dot(x,theta)\n",
        "  return J\n",
        "x, theta = 2, 4\n",
        "J = forward_propagation(x, theta)\n",
        "print (\"J = \" + str(J))  \n",
        "def backward_propagation(x,theta):\n",
        "  dtheta = x\n",
        "  return dtheta\n",
        "x, theta = 2, 4\n",
        "dtheta = backward_propagation(x, theta)\n",
        "print (\"dtheta = \" + str(dtheta))    \n",
        "def gradient_check(x,theta,epsilon=1e-7):\n",
        "  thetaplus = theta + epsilon\n",
        "  thetaminus = theta - epsilon\n",
        "  J_plus = forward_propagation(x,thetaplus)\n",
        "  J_minus = backward_propagation(x,thetaminus)\n",
        "  gradapprox = (J_plus - J_minus)/ (2 * epsilon)\n",
        "  grad = backward_propagation(x,theta)\n",
        "  numerator = np.linalg.norm(grad - gradapprox)\n",
        "  denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
        "  difference = numerator/denominator\n",
        "  if difference < 1e-7:\n",
        "    print(\"the gradient is right\")\n",
        "  else:\n",
        "    print(\"the gradient is wrong\")\n",
        "  return difference\n",
        "x,theta = 2,4\n",
        "difference = gradient_check(x,theta)\n",
        "print(\"difference is: \" +str(difference))  \n",
        "def forward_propagation_n(X,Y,parameters):\n",
        "  m = X.shape[1]\n",
        "  W1 = parameters[\"W1\"]\n",
        "  b1 = parameters[\"b1\"]\n",
        "  W2 = parameters[\"W2\"]\n",
        "  b2 = parameters[\"b2\"]\n",
        "  W3 = parameters[\"W3\"]\n",
        "  b3 = parameters[\"b3\"]\n",
        "  Z1 = np.dot(W1,X) + b1\n",
        "  A1 = relu(Z1)\n",
        "  Z2 = np.dot(W2,A1) + b2\n",
        "  A2 = relu(Z2)\n",
        "  Z3 = np.dot(W3,A2) + b3\n",
        "  A3 = sigmoid(Z3)\n",
        "  logprobs = np.multiply(Y,-np.log(A3)) + np.multiply(1-Y,-np.log(1 - A3))\n",
        "  cost = 1/m * np.sum(logprobs)\n",
        "  cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
        "    \n",
        "  return cost, cache\n",
        "\n",
        "def backward_propagation_n(X,Y,cache):\n",
        "  m = X.shape[1]\n",
        "  (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
        "  dZ3 = A3 - Y\n",
        "  dW3 = 1/m * np.dot(dZ3,A2.T)\n",
        "  db3 = 1/m * np.sum(dZ3,axis = 1,keepdims = True)\n",
        "  dA2 = np.dot(W3.T,dZ3)\n",
        "  dZ2 = np.multiply(dA2,np.int64(A2 > 0))\n",
        "  dW2 = 1./m * np.dot(dZ2, A1.T) * 2\n",
        "  db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
        "    \n",
        "  dA1 = np.dot(W2.T, dZ2)\n",
        "  dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
        "  dW1 = 1./m * np.dot(dZ1, X.T)\n",
        "  db1 = 4./m * np.sum(dZ1, axis=1, keepdims = True)\n",
        "    \n",
        "  gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
        "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
        "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
        "    \n",
        "  return gradients \n",
        "def gradient_check_n(parameters,gradients,X,Y,epsilon = 1e-7):\n",
        "  parameters_values,_ = dictionary_to_vector(parameters)\n",
        "  grad = gradients_to_vector(gradients)\n",
        "  num_parameters = parameters_values.shape[0]\n",
        "  J_plus = np.zeros((num_parameters,1))\n",
        "  J_minus = np.zeros((num_parameters,1))\n",
        "  gradapprox = np.zeros((num_parameters,1))\n",
        "  for i in range(num_parameters):\n",
        "    thetaplus = np.copy(parameters_values)\n",
        "    thetaplus[i][0] = thetaplus[i][0] + epsilon\n",
        "    J_plus[i],_ = forward_propagation_n(X,Y,vector_to_dictionary(thetaplus))\n",
        "    thetaminus = np.copy(parameters_values)\n",
        "    thetaminus[i][0] = thetaminus[i][0] + epsilon\n",
        "    J_minus[i],_ = forward_propagation_n(X,Y,vector_to_dictionary(thetaminus))\n",
        "    gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
        "    numerator = np.linalg.norm(grad - gradapprox)\n",
        "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
        "    difference = numerator/denominator\n",
        "    if difference > 2e-7:\n",
        "        print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
        "    else:\n",
        "        print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
        "    \n",
        "    return difference\n",
        "X, Y, parameters = gradient_check_n_test_case()\n",
        "\n",
        "cost, cache = forward_propagation_n(X, Y, parameters)\n",
        "gradients = backward_propagation_n(X, Y, cache)\n",
        "difference = gradient_check_n(parameters, gradients, X, Y)\n",
        "def backward_propagation_n(X,Y,cache):\n",
        "    m = X.shape[1]\n",
        "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
        "    \n",
        "    dZ3 = A3 - Y\n",
        "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
        "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
        "    \n",
        "    dA2 = np.dot(W3.T, dZ3)\n",
        "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
        "    dW2 = 1./m * np.dot(dZ2, A1.T)                                # Should not multiply by 2\n",
        "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
        "    \n",
        "    dA1 = np.dot(W2.T, dZ2)\n",
        "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
        "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
        "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)             # Should not multiply by 4\n",
        "    \n",
        "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
        "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
        "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
        "    \n",
        "    return gradients\n",
        "X, Y, parameters = gradient_check_n_test_case()\n",
        "\n",
        "cost, cache = forward_propagation_n(X, Y, parameters)\n",
        "gradients = backward_propagation_n(X, Y, cache)\n",
        "difference = gradient_check_n(parameters, gradients, X, Y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "J = 8\n",
            "dtheta = 2\n",
            "the gradient is wrong\n",
            "difference is: 0.99999986666668\n",
            "\u001b[93mThere is a mistake in the backward propagation! difference = 1.0\u001b[0m\n",
            "\u001b[93mThere is a mistake in the backward propagation! difference = 1.0\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afoyePAZbowd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}